# d:\TIEN\Nam5\DATN\EduMark\backend\ocr_llm\llm_processor.py
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import os
import re
import json
import PIL.Image
from dotenv import load_dotenv
from prompt import prompt_template
from concurrent.futures import ThreadPoolExecutor, as_completed
# from encoding_fix_backup import force_utf8
# force_utf8()

dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# --- Singleton Pattern for Gemini Model ---
_gemini_model = None

def _configure_gemini():
    global _gemini_model
    try:
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("ERROR: Environment variable 'GOOGLE_API_KEY' is not set in .env file.")
        
        genai.configure(api_key=api_key)
        
        print("Initializing Gemini (gemini-Flash-lastest) model...")
        # S·ª≠ d·ª•ng gemini-pro, m·ªôt model m·∫°nh m·∫Ω v√† ·ªïn ƒë·ªãnh
        _gemini_model = genai.GenerativeModel('gemini-flash-latest') 
        print("‚úÖ Gemini model is ready.")
        
    except Exception as e:
        print(f"üõë Error in _configure_gemini: {e}")
        # N√©m l·∫°i l·ªói ƒë·ªÉ d·ª´ng ch∆∞∆°ng tr√¨nh n·∫øu kh√¥ng th·ªÉ k·∫øt n·ªëi Gemini
        raise

def get_gemini_model():
    global _gemini_model
    if _gemini_model is None:
        _configure_gemini()
    return _gemini_model
def clean_json_string(text):
    """
    H√†m l√†m s·∫°ch chu·ªói JSON tr·∫£ v·ªÅ t·ª´ LLM.
    Lo·∫°i b·ªè c√°c k√Ω t·ª± markdown nh∆∞ ```json ... ```
    """
    if not text:
        return ""
    
    # 1. Lo·∫°i b·ªè markdown code block
    text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'```$', '', text, flags=re.MULTILINE)
    
    # 2. Trim kho·∫£ng tr·∫Øng
    text = text.strip()
    return text

# --- Main Grading Function ---
def grade_submission_with_llm(image_paths: list, rubric: str, final_context_text: str):
    
    print("\n--- 4. START GRADING WITH LLM ---")
    
    # Tr∆∞·ªùng h·ª£p OCR kh√¥ng ƒë·ªçc ƒë∆∞·ª£c g√¨
    if not image_paths:
        print("‚ö†Ô∏è Warning: Empty OCR text, cannot be graded.")
        return {
            "score": 0,
            "comment": "Kh√¥ng t√¨m th·∫•y b√†i l√†m ƒë·ªÉ ch·∫•m",
            "feasibility": False,
            "details": {}
        }

    try:
        # L·∫•y model Gemini (s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o n·∫øu ch∆∞a c√≥)
        model = get_gemini_model()

        image_parts = []
        for path in image_paths:
            try:
                if os.path.exists(path):
                    img = PIL.Image.open(path)
                    image_parts.append(img)
                    print(f"‚úÖ Loaded image for Vision: {os.path.basename(path)}")
                else:
                    print(f"‚ö†Ô∏è Image path not found: {path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading image {path}: {e}")

        if not image_parts:
            return {
                "score": 0,
                "comment": "L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c file ·∫£nh n√†o.",
                "feasibility": False,
                "details": {}
            }

        # ƒêi·ªÅn th√¥ng tin v√†o prompt template
        final_prompt = prompt_template.format(
            rubric=rubric,
            recognized_text=final_context_text
        )

        # 4. G·ª≠i Request ƒêa ph∆∞∆°ng th·ª©c (Multimodal: Text Prompt + Images)
        # Gemini nh·∫≠n input l√† m·ªôt list [Prompt_Text, Image1, Image2, ...]
        input_content = [final_prompt] + image_parts

        print("Sending VISION request (Text + Images) to Google API...")

        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }

        # C·∫•u h√¨nh ƒë·ªÉ y√™u c·∫ßu LLM tr·∫£ v·ªÅ ƒë√∫ng ƒë·ªãnh d·∫°ng JSON
        generation_config = genai.GenerationConfig(
            response_mime_type="application/json",
            temperature=0.1# Gi·∫£m s√°ng t·∫°o ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
        )

        print("Sending score request to Google API...")
        # G·ªçi API v√† l·∫•y k·∫øt qu·∫£
        response = model.generate_content(
            input_content,
            generation_config=generation_config
        )

        # Tr√≠ch xu·∫•t n·ªôi dung text t·ª´ response m·ªôt c√°ch an to√†n
        import traceback as _tb
        response_text = None
        try:
            # Tr∆∞·ªùng h·ª£p th√¥ng th∆∞·ªùng: response.candidates[0].content.parts[0].text
            if getattr(response, 'candidates', None) and len(response.candidates) > 0:
                cand = response.candidates[0]
                # Try common attribute paths used by different SDK versions
                try:
                    response_text = cand.content.parts[0].text
                except Exception:
                    try:
                        response_text = cand.output[0].content[0].text
                    except Exception:
                        # last resort: stringify candidate
                        response_text = None

            # If still no text, try to serialize response for debugging
            if not response_text:
                print("‚ö†Ô∏è Could not extract text via expected fields. Dumping response for debug...")
                try:
                    # response may be a proto message; convert to string
                    print(repr(response))
                except Exception:
                    print("(failed to repr response)")
                raise ValueError("No textual candidate found in API response")

            print("‚úÖGet JSON response from API.")
            # Parse chu·ªói JSON th√†nh dictionary c·ªßa Python
            cleaned_json = clean_json_string(response_text)
            grading_result = json.loads(cleaned_json)
            print("‚úÖ JSON parsed successfully.")
            return grading_result
        except Exception as e:
            print(f"üõë Failed to extract/parse JSON from LLM response: {e}")
            _tb.print_exc()
            return {
                "score": 0,
                "comment": f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ LLM: {e}",
                "feasibility": False,
                "details": {"raw_response": str(response)[:2000]}
            }

    except Exception as e:
        import traceback
        print(f"üõë Serious ERROR in grading LLM: {e}")
        traceback.print_exc()
        # Tr·∫£ v·ªÅ m·ªôt c·∫•u tr√∫c l·ªói nh·∫•t qu√°n
        return {
            "score": 0,
            "comment": f"ƒê√£ x·∫£y ra l·ªói h·ªá th·ªëng trong qu√° tr√¨nh ch·∫•m ƒëi·ªÉm b·∫±ng AI: {e}",
            "feasibility": False,
            "details": {}
        }

def grade_multiple_submissions_parallel(submissions, max_workers=5):
    """
    Ch·∫•m nhi·ªÅu b√†i song song b·∫±ng Gemini (ThreadPoolExecutor)
    submissions: List tuple (image_paths, rubric, final_context_text)
    max_workers: s·ªë lu·ªìng x·ª≠ l√Ω song song
    """

    results = [None] * len(submissions)

    print(f"\nüöÄ Parallel LLM grading started with {max_workers} threads...")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {
            executor.submit(grade_submission_with_llm, imgs, rub, txt): i
            for i, (imgs, rub, txt) in enumerate(submissions)
        }

        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                results[idx] = future.result()
                print(f"‚úÖ Finished grading submission #{idx+1}")
            except Exception as e:
                print(f"‚ùå Error grading submission #{idx+1}: {e}")
                results[idx] = {
                    "score": 0,
                    "comment": "L·ªói lu·ªìng x·ª≠ l√Ω song song.",
                    "feasibility": False,
                    "details": {}
                }

    print("üéâ All parallel grading completed.")
    return results

