# d:\TIEN\Nam5\DATN\EduMark\backend\ocr_llm\llm_processor.py
import google.generativeai as genai
import os
import json
from dotenv import load_dotenv
from prompt import prompt_template
# from encoding_fix_backup import force_utf8
# force_utf8()

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env ·ªü th∆∞ m·ª•c backend
# ƒêi·ªÅu n√†y gi√∫p qu·∫£n l√Ω API key m·ªôt c√°ch an to√†n
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# --- Singleton Pattern for Gemini Model ---
_gemini_model = None

def _configure_gemini():
    global _gemini_model
    try:
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("ERROR: Environment variable 'GOOGLE_API_KEY' is not set in .env file.")
        
        genai.configure(api_key=api_key)
        
        print("Initializing Gemini (gemini-Flash-lastest) model...")
        # S·ª≠ d·ª•ng gemini-pro, m·ªôt model m·∫°nh m·∫Ω v√† ·ªïn ƒë·ªãnh
        _gemini_model = genai.GenerativeModel('gemini-flash-latest') 
        print("‚úÖ Gemini model is ready.")
        
    except Exception as e:
        print(f"üõë Error in _configure_gemini: {e}")
        # N√©m l·∫°i l·ªói ƒë·ªÉ d·ª´ng ch∆∞∆°ng tr√¨nh n·∫øu kh√¥ng th·ªÉ k·∫øt n·ªëi Gemini
        raise

def get_gemini_model():
    global _gemini_model
    if _gemini_model is None:
        _configure_gemini()
    return _gemini_model

# --- Main Grading Function ---
def grade_submission_with_llm(recognized_text: str, rubric: str):
    
    print("\n--- 4. START GRADING WITH LLM ---")
    
    # Tr∆∞·ªùng h·ª£p OCR kh√¥ng ƒë·ªçc ƒë∆∞·ª£c g√¨
    if not recognized_text or not recognized_text.strip():
        print("‚ö†Ô∏è Warning: Empty OCR text, cannot be graded.")
        return {
            "score": 0,
            "comment": "Kh√¥ng th·ªÉ ch·∫•m ƒëi·ªÉm do kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ ·∫£nh b√†i l√†m.",
            "feasibility": False,
            "details": {}
        }

    try:
        # L·∫•y model Gemini (s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o n·∫øu ch∆∞a c√≥)
        model = get_gemini_model()


        # ƒêi·ªÅn th√¥ng tin v√†o prompt template
        final_prompt = prompt_template.format(
            rubric=rubric,
            recognized_text=recognized_text
        )

        # C·∫•u h√¨nh ƒë·ªÉ y√™u c·∫ßu LLM tr·∫£ v·ªÅ ƒë√∫ng ƒë·ªãnh d·∫°ng JSON
        generation_config = genai.GenerationConfig(
            response_mime_type="application/json"
        )

        print("Sending score request to Google API...")
        # G·ªçi API v√† l·∫•y k·∫øt qu·∫£
        response = model.generate_content(
            [final_prompt],
            generation_config=generation_config
        )

        # Tr√≠ch xu·∫•t n·ªôi dung text t·ª´ response m·ªôt c√°ch an to√†n
        import traceback as _tb
        response_text = None
        try:
            # Tr∆∞·ªùng h·ª£p th√¥ng th∆∞·ªùng: response.candidates[0].content.parts[0].text
            if getattr(response, 'candidates', None) and len(response.candidates) > 0:
                cand = response.candidates[0]
                # Try common attribute paths used by different SDK versions
                try:
                    response_text = cand.content.parts[0].text
                except Exception:
                    try:
                        response_text = cand.output[0].content[0].text
                    except Exception:
                        # last resort: stringify candidate
                        response_text = None

            # If still no text, try to serialize response for debugging
            if not response_text:
                print("‚ö†Ô∏è Could not extract text via expected fields. Dumping response for debug...")
                try:
                    # response may be a proto message; convert to string
                    print(repr(response))
                except Exception:
                    print("(failed to repr response)")
                raise ValueError("No textual candidate found in API response")

            print("‚úÖGet JSON response from API.")
            # Parse chu·ªói JSON th√†nh dictionary c·ªßa Python
            grading_result = json.loads(response_text)
            print("‚úÖ JSON parsed successfully.")
            return grading_result
        except Exception as e:
            print(f"üõë Failed to extract/parse JSON from LLM response: {e}")
            _tb.print_exc()
            return {
                "score": 0,
                "comment": f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ LLM: {e}",
                "feasibility": False,
                "details": {"raw_response": str(response)[:2000]}
            }

    except Exception as e:
        import traceback
        print(f"üõë Serious ERROR in grading LLM: {e}")
        traceback.print_exc()
        # Tr·∫£ v·ªÅ m·ªôt c·∫•u tr√∫c l·ªói nh·∫•t qu√°n
        return {
            "score": 0,
            "comment": f"ƒê√£ x·∫£y ra l·ªói h·ªá th·ªëng trong qu√° tr√¨nh ch·∫•m ƒëi·ªÉm b·∫±ng AI: {e}",
            "feasibility": False,
            "details": {}
        }
from concurrent.futures import ThreadPoolExecutor, as_completed

def grade_multiple_submissions_parallel(submissions, max_workers=5):
    """
    Ch·∫•m nhi·ªÅu b√†i song song b·∫±ng Gemini (ThreadPoolExecutor)

    submissions: List tuple (recognized_text, rubric)
    max_workers: s·ªë lu·ªìng x·ª≠ l√Ω song song

    Returns:
        List k·∫øt qu·∫£ ch·∫•m ƒëi·ªÉm (gi·ªØ ƒë√∫ng th·ª© t·ª± input)
    """

    results = [None] * len(submissions)

    print(f"\nüöÄ Parallel LLM grading started with {max_workers} threads...")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {
            executor.submit(grade_submission_with_llm, text, rubric): i
            for i, (text, rubric) in enumerate(submissions)
        }

        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                results[idx] = future.result()
                print(f"‚úÖ Finished grading submission #{idx+1}")
            except Exception as e:
                print(f"‚ùå Error grading submission #{idx+1}: {e}")
                results[idx] = {
                    "score": 0,
                    "comment": "L·ªói khi ch·∫•m b√†i b·∫±ng AI",
                    "feasibility": False,
                    "details": {}
                }

    print("üéâ All parallel grading completed.")
    return results

